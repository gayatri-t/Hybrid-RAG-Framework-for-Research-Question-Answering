{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2cfbeab-5aa6-48bd-88ab-8477d65db541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sentence_transformers import CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class HybridResearchRAG:\n",
    "    \"\"\"\n",
    "    Advanced RAG system with reranking, hybrid search, and caching.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        excel_path: Optional[str] = None,\n",
    "        tavily_api_key: Optional[str] = None,\n",
    "        vector_store_path: str = \"research_vectorstore\",\n",
    "        model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        cache_path: str = \"query_cache.json\",\n",
    "        use_reranker: bool = True,\n",
    "        use_hybrid_search: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid RAG system.\n",
    "        \n",
    "        Args:\n",
    "            excel_path: Path to Excel file with paper metadata\n",
    "            tavily_api_key: API key for Tavily web search\n",
    "            vector_store_path: Directory to save/load vector store\n",
    "            model_name: HuggingFace model for generation\n",
    "            cache_path: Path to query cache file\n",
    "            use_reranker: Whether to use cross-encoder reranking\n",
    "            use_hybrid_search: Whether to use BM25 + vector hybrid search\n",
    "        \"\"\"\n",
    "        self.vector_store_path = vector_store_path\n",
    "        self.excel_path = excel_path\n",
    "        self.tavily_api_key = tavily_api_key\n",
    "        self.model_name = model_name\n",
    "        self.cache_path = cache_path\n",
    "        self.use_reranker = use_reranker\n",
    "        self.use_hybrid_search = use_hybrid_search\n",
    "        \n",
    "        # Initialize query cache\n",
    "        self.query_cache = self._load_cache()\n",
    "        self.cache_hits = 0\n",
    "        self.total_queries = 0\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        print(\"Loading embedding model...\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={'device': 'cpu'}\n",
    "        )\n",
    "        \n",
    "        # Initialize reranker if enabled\n",
    "        self.reranker = None\n",
    "        if use_reranker:\n",
    "            print(\"Loading reranker model...\")\n",
    "            self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "        \n",
    "        # Initialize text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=600,\n",
    "            chunk_overlap=75,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \" \"],\n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "        # Initialize LLM\n",
    "        print(f\"Loading generation model: {model_name}...\")\n",
    "        self.llm = self._initialize_llm(model_name)\n",
    "        \n",
    "        # Initialize vector store and BM25 index\n",
    "        self.vectorstore = None\n",
    "        self.bm25_index = None\n",
    "        self.bm25_docs = []\n",
    "        self.load_or_create_vectorstore()\n",
    "        \n",
    "        # Initialize online retriever\n",
    "        self.online_retriever = None\n",
    "        if tavily_api_key:\n",
    "            self.online_retriever = TavilySearchAPIRetriever(\n",
    "                k=6,\n",
    "                api_key=tavily_api_key\n",
    "            )\n",
    "    \n",
    "    def _load_cache(self) -> Dict:\n",
    "        \"\"\"Load query cache from disk.\"\"\"\n",
    "        if os.path.exists(self.cache_path):\n",
    "            try:\n",
    "                with open(self.cache_path, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except:\n",
    "                return {}\n",
    "        return {}\n",
    "    \n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save query cache to disk.\"\"\"\n",
    "        try:\n",
    "            with open(self.cache_path, 'w') as f:\n",
    "                json.dump(self.query_cache, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save cache: {e}\")\n",
    "    \n",
    "    def _get_cache_key(self, question: str, search_online: bool) -> str:\n",
    "        \"\"\"Generate cache key for a query.\"\"\"\n",
    "        key_str = f\"{question.lower().strip()}_{search_online}\"\n",
    "        return hashlib.md5(key_str.encode()).hexdigest()\n",
    "    \n",
    "    def _initialize_llm(self, model_name: str):\n",
    "        \"\"\"Initialize the language model.\"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.6,\n",
    "            top_p=0.85,\n",
    "            repetition_penalty=1.2,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            return_full_text=False,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        return HuggingFacePipeline(pipeline=pipe)\n",
    "    \n",
    "    def load_papers_from_excel(self) -> List[Dict]:\n",
    "        \"\"\"Load paper metadata from Excel file.\"\"\"\n",
    "        if not self.excel_path or not os.path.exists(self.excel_path):\n",
    "            return []\n",
    "        \n",
    "        df = pd.read_excel(self.excel_path)\n",
    "        return [\n",
    "            {\n",
    "                \"title\": row[\"Title\"],\n",
    "                \"url\": row[\"Link\"],\n",
    "                \"s_no\": row.get(\"S.No\", idx)\n",
    "            }\n",
    "            for idx, row in df.iterrows()\n",
    "        ]\n",
    "    \n",
    "    def load_documents_from_papers(self, papers: List[Dict]) -> List:\n",
    "        \"\"\"Load and process documents from paper URLs.\"\"\"\n",
    "        all_docs = []\n",
    "        total = len(papers)\n",
    "        \n",
    "        print(f\"\\nLoading {total} research papers...\")\n",
    "        \n",
    "        for idx, paper in enumerate(papers, 1):\n",
    "            if idx % 50 == 0:\n",
    "                print(f\"Progress: {idx}/{total} papers loaded\")\n",
    "            \n",
    "            try:\n",
    "                if paper[\"url\"].endswith('.pdf'):\n",
    "                    loader = PyMuPDFLoader(paper[\"url\"])\n",
    "                else:\n",
    "                    loader = WebBaseLoader(paper[\"url\"])\n",
    "                \n",
    "                docs = loader.load()\n",
    "                \n",
    "                for doc in docs:\n",
    "                    doc.metadata.update({\n",
    "                        \"paper_title\": paper[\"title\"],\n",
    "                        \"paper_url\": paper[\"url\"],\n",
    "                        \"s_no\": paper[\"s_no\"],\n",
    "                        \"source_type\": \"local_collection\"\n",
    "                    })\n",
    "                \n",
    "                all_docs.extend(docs)\n",
    "                \n",
    "            except Exception as e:\n",
    "                with open(\"load_errors.log\", \"a\") as f:\n",
    "                    f.write(f\"Failed: {paper['title']} - {str(e)}\\n\")\n",
    "        \n",
    "        print(f\"Successfully loaded {len(all_docs)} documents\")\n",
    "        return all_docs\n",
    "    \n",
    "    def create_bm25_index(self, documents):\n",
    "        \"\"\"Create BM25 index for keyword search.\"\"\"\n",
    "        print(\"Creating BM25 index...\")\n",
    "        self.bm25_docs = documents\n",
    "        tokenized = [doc.page_content.lower().split() for doc in documents]\n",
    "        self.bm25_index = BM25Okapi(tokenized)\n",
    "        print(\"BM25 index created\")\n",
    "    \n",
    "    def load_or_create_vectorstore(self):\n",
    "        \"\"\"Load existing vector store or create new one.\"\"\"\n",
    "        index_path = f\"{self.vector_store_path}/index.faiss\"\n",
    "        \n",
    "        if os.path.exists(index_path):\n",
    "            print(\"Loading existing vector store...\")\n",
    "            self.vectorstore = FAISS.load_local(\n",
    "                self.vector_store_path,\n",
    "                self.embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            print(\"Vector store loaded successfully\")\n",
    "            \n",
    "            # Load BM25 index if hybrid search enabled\n",
    "            if self.use_hybrid_search:\n",
    "                # Get all documents from vectorstore for BM25\n",
    "                # This is a workaround - ideally save/load BM25 separately\n",
    "                print(\"Note: BM25 index will be created on first hybrid search\")\n",
    "                \n",
    "        elif self.excel_path:\n",
    "            print(\"Creating new vector store from local papers...\")\n",
    "            papers = self.load_papers_from_excel()\n",
    "            if papers:\n",
    "                docs = self.load_documents_from_papers(papers)\n",
    "                chunks = self.text_splitter.split_documents(docs)\n",
    "                \n",
    "                print(f\"Creating embeddings for {len(chunks)} chunks...\")\n",
    "                self.vectorstore = FAISS.from_documents(chunks, self.embeddings)\n",
    "                self.vectorstore.save_local(self.vector_store_path)\n",
    "                \n",
    "                # Create BM25 index if enabled\n",
    "                if self.use_hybrid_search:\n",
    "                    self.create_bm25_index(chunks)\n",
    "                \n",
    "                print(\"Vector store created and saved\")\n",
    "            else:\n",
    "                print(\"No papers found to process\")\n",
    "        else:\n",
    "            print(\"No vector store or Excel file provided - online search only mode\")\n",
    "    \n",
    "    def rerank_documents(self, query: str, docs: List, top_k: int = 5):\n",
    "        \"\"\"Rerank retrieved documents using cross-encoder.\"\"\"\n",
    "        if not docs or not self.reranker:\n",
    "            return docs\n",
    "        \n",
    "        pairs = [[query, doc.page_content] for doc in docs]\n",
    "        scores = self.reranker.predict(pairs)\n",
    "        \n",
    "        scored_docs = list(zip(docs, scores))\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return [doc for doc, _ in scored_docs[:top_k]]\n",
    "    \n",
    "    def hybrid_retrieve(self, query: str, k: int = 10):\n",
    "        \"\"\"Combine BM25 and vector search for better retrieval.\"\"\"\n",
    "        if not self.use_hybrid_search or not self.vectorstore:\n",
    "            # Fallback to vector search only\n",
    "            return self.vectorstore.similarity_search(query, k=k)\n",
    "        \n",
    "        # Initialize BM25 if not done yet\n",
    "        if self.bm25_index is None:\n",
    "            # Get all docs from vectorstore (expensive, should be cached)\n",
    "            all_docs = self.vectorstore.similarity_search(\"\", k=10000)  # Hack to get all\n",
    "            if all_docs:\n",
    "                self.create_bm25_index(all_docs)\n",
    "            else:\n",
    "                return self.vectorstore.similarity_search(query, k=k)\n",
    "        \n",
    "        # BM25 search\n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = self.bm25_index.get_scores(tokenized_query)\n",
    "        bm25_top_indices = np.argsort(bm25_scores)[-k*2:][::-1]  # Get 2x more for fusion\n",
    "        \n",
    "        # Vector search\n",
    "        vector_docs = self.vectorstore.similarity_search(query, k=k*2)\n",
    "        \n",
    "        # Combine results (simple approach: union and deduplicate)\n",
    "        combined_docs = {}\n",
    "        \n",
    "        # Add BM25 results with scores\n",
    "        for idx in bm25_top_indices:\n",
    "            if idx < len(self.bm25_docs):\n",
    "                doc = self.bm25_docs[idx]\n",
    "                doc_key = doc.page_content[:100]\n",
    "                combined_docs[doc_key] = doc\n",
    "        \n",
    "        # Add vector results\n",
    "        for doc in vector_docs:\n",
    "            doc_key = doc.page_content[:100]\n",
    "            combined_docs[doc_key] = doc\n",
    "        \n",
    "        # Return top k unique documents\n",
    "        return list(combined_docs.values())[:k]\n",
    "    \n",
    "    def search_online(self, query: str) -> List:\n",
    "        \"\"\"Search online sources using Tavily.\"\"\"\n",
    "        if not self.online_retriever:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            print(\"Searching online sources...\")\n",
    "            docs = self.online_retriever.invoke(query)\n",
    "            \n",
    "            for doc in docs:\n",
    "                doc.metadata[\"source_type\"] = \"online_search\"\n",
    "                if \"title\" in doc.metadata:\n",
    "                    doc.metadata[\"paper_title\"] = doc.metadata[\"title\"]\n",
    "                elif \"source\" in doc.metadata:\n",
    "                    from urllib.parse import urlparse\n",
    "                    domain = urlparse(doc.metadata[\"source\"]).netloc.replace(\"www.\", \"\")\n",
    "                    doc.metadata[\"paper_title\"] = f\"Web: {domain}\"\n",
    "                else:\n",
    "                    doc.metadata[\"paper_title\"] = \"Online Source\"\n",
    "            \n",
    "            chunks = self.text_splitter.split_documents(docs)\n",
    "            return chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Online search error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _get_prompt_template(self) -> PromptTemplate:\n",
    "        \"\"\"Get the prompt template.\"\"\"\n",
    "        template = \"\"\"Use the research papers below to answer the question. Write 2-3 clear sentences that directly answer what is asked. Cite paper titles in [brackets].\n",
    "\n",
    "Research Papers:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Direct answer:\"\"\"\n",
    "        \n",
    "        return PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "    \n",
    "    def create_qa_chain(self, retriever):\n",
    "        \"\"\"Create the QA chain with given retriever.\"\"\"\n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            chain_type_kwargs={\"prompt\": self._get_prompt_template()},\n",
    "            return_source_documents=True,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    def _clean_answer(self, raw_answer: str) -> str:\n",
    "        \"\"\"Clean up model output.\"\"\"\n",
    "        cleaned = raw_answer.strip()\n",
    "        \n",
    "        placeholders = [\n",
    "            \"[Insert your response here]\",\n",
    "            \"[Your answer here]\",\n",
    "            \"[Response]\"\n",
    "        ]\n",
    "        for placeholder in placeholders:\n",
    "            cleaned = cleaned.replace(placeholder, \"\").strip()\n",
    "        \n",
    "        prefixes_to_remove = [\n",
    "            \"Use the research papers below\",\n",
    "            \"Research Papers:\",\n",
    "            \"Question:\",\n",
    "            \"Direct answer:\",\n",
    "            \"Answer:\",\n",
    "        ]\n",
    "        \n",
    "        for prefix in prefixes_to_remove:\n",
    "            if prefix in cleaned:\n",
    "                parts = cleaned.split(prefix)\n",
    "                cleaned = parts[-1].strip()\n",
    "        \n",
    "        stop_phrases = [\"\\nNote:\", \"\\nReferences:\", \"\\nBest regards\", \"\\nPlease\", \"\\n\\n\"]\n",
    "        for phrase in stop_phrases:\n",
    "            if phrase in cleaned:\n",
    "                cleaned = cleaned.split(phrase)[0].strip()\n",
    "        \n",
    "        if len(cleaned) < 10:\n",
    "            return \"The provided sources don't contain sufficient information to answer this question.\"\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    def query(self, question: str, search_online: bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Query the RAG system with caching.\n",
    "        \"\"\"\n",
    "        self.total_queries += 1\n",
    "        \n",
    "        # Check cache first\n",
    "        cache_key = self._get_cache_key(question, search_online)\n",
    "        if cache_key in self.query_cache:\n",
    "            self.cache_hits += 1\n",
    "            print(\"✓ Using cached result\")\n",
    "            return self.query_cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            # Handle online search\n",
    "            if search_online and self.online_retriever:\n",
    "                online_docs = self.search_online(question)\n",
    "                \n",
    "                if online_docs:\n",
    "                    # Apply reranking if enabled\n",
    "                    if self.use_reranker:\n",
    "                        print(\"Reranking online results...\")\n",
    "                        online_docs = self.rerank_documents(question, online_docs, top_k=5)\n",
    "                    \n",
    "                    temp_store = FAISS.from_documents(online_docs, self.embeddings)\n",
    "                    retriever = temp_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "                    qa_chain = self.create_qa_chain(retriever)\n",
    "                    response = qa_chain.invoke({\"query\": question})\n",
    "                else:\n",
    "                    result = {\n",
    "                        \"answer\": \"No relevant online sources found.\",\n",
    "                        \"sources\": []\n",
    "                    }\n",
    "                    self.query_cache[cache_key] = result\n",
    "                    self._save_cache()\n",
    "                    return result\n",
    "            else:\n",
    "                # Use local vectorstore\n",
    "                if not self.vectorstore:\n",
    "                    result = {\n",
    "                        \"answer\": \"No local papers available. Use --online to search the web.\",\n",
    "                        \"sources\": []\n",
    "                    }\n",
    "                    return result\n",
    "                \n",
    "                # Retrieve using hybrid search or regular search\n",
    "                if self.use_hybrid_search:\n",
    "                    print(\"Using hybrid search (BM25 + Vector)...\")\n",
    "                    docs = self.hybrid_retrieve(question, k=10)\n",
    "                else:\n",
    "                    docs = self.vectorstore.similarity_search(question, k=8)\n",
    "                \n",
    "                # Apply reranking if enabled\n",
    "                if self.use_reranker and docs:\n",
    "                    print(\"Reranking results...\")\n",
    "                    docs = self.rerank_documents(question, docs, top_k=5)\n",
    "                \n",
    "                # Create retriever from filtered docs\n",
    "                temp_store = FAISS.from_documents(docs, self.embeddings)\n",
    "                retriever = temp_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "                qa_chain = self.create_qa_chain(retriever)\n",
    "                response = qa_chain.invoke({\"query\": question})\n",
    "            \n",
    "            # Extract and clean answer\n",
    "            raw_answer = response.get(\"result\", \"No answer generated\").strip()\n",
    "            answer = self._clean_answer(raw_answer)\n",
    "            \n",
    "            # Process sources\n",
    "            sources = []\n",
    "            seen = set()\n",
    "            \n",
    "            for doc in response.get(\"source_documents\", []):\n",
    "                source_info = {\n",
    "                    \"title\": doc.metadata.get(\"paper_title\", \"Unknown\"),\n",
    "                    \"url\": doc.metadata.get(\"paper_url\", doc.metadata.get(\"source\", \"#\")),\n",
    "                    \"type\": doc.metadata.get(\"source_type\", \"unknown\"),\n",
    "                    \"page\": doc.metadata.get(\"page\", \"N/A\")\n",
    "                }\n",
    "                \n",
    "                identifier = f\"{source_info['title']}_{source_info['url']}\"\n",
    "                if identifier not in seen:\n",
    "                    sources.append(source_info)\n",
    "                    seen.add(identifier)\n",
    "            \n",
    "            result = {\n",
    "                \"answer\": answer,\n",
    "                \"sources\": sources\n",
    "            }\n",
    "            \n",
    "            # Cache the result\n",
    "            self.query_cache[cache_key] = result\n",
    "            self._save_cache()\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            return {\n",
    "                \"answer\": f\"Error processing query: {str(e)}\\n{traceback.format_exc()}\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"Get cache performance statistics.\"\"\"\n",
    "        if self.total_queries > 0:\n",
    "            hit_rate = (self.cache_hits / self.total_queries) * 100\n",
    "            return f\"Cache: {self.cache_hits}/{self.total_queries} hits ({hit_rate:.1f}%)\"\n",
    "        return \"Cache: No queries yet\"\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the query cache.\"\"\"\n",
    "        self.query_cache = {}\n",
    "        self._save_cache()\n",
    "        self.cache_hits = 0\n",
    "        self.total_queries = 0\n",
    "        print(\"Cache cleared\")\n",
    "    \n",
    "    def interactive_session(self):\n",
    "        \"\"\"Run interactive Q&A session.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Research Assistant - Interactive Mode\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Commands:\")\n",
    "        print(\"  - Type your question to search local papers\")\n",
    "        print(\"  - Add '--online' to search online sources\")\n",
    "        print(\"  - Type 'stats' to see cache statistics\")\n",
    "        print(\"  - Type 'clear-cache' to clear cache\")\n",
    "        print(\"  - Type 'exit' to quit\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nQuestion: \").strip()\n",
    "                \n",
    "                if user_input.lower() in ['exit', 'quit', 'q']:\n",
    "                    print(f\"\\n{self.get_cache_stats()}\")\n",
    "                    print(\"Goodbye!\")\n",
    "                    break\n",
    "                \n",
    "                if user_input.lower() == 'stats':\n",
    "                    print(f\"\\n{self.get_cache_stats()}\")\n",
    "                    continue\n",
    "                \n",
    "                if user_input.lower() == 'clear-cache':\n",
    "                    self.clear_cache()\n",
    "                    continue\n",
    "                \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                # Check for online flag\n",
    "                search_online = '--online' in user_input\n",
    "                question = user_input.replace('--online', '').strip()\n",
    "                \n",
    "                print(\"\\nProcessing...\\n\")\n",
    "                result = self.query(question, search_online=search_online)\n",
    "                \n",
    "                # Display results\n",
    "                print(f\"Answer: {result['answer']}\\n\")\n",
    "                \n",
    "                if result['sources']:\n",
    "                    print(\"Sources:\")\n",
    "                    for idx, source in enumerate(result['sources'], 1):\n",
    "                        print(f\"\\n[{idx}] {source['title']}\")\n",
    "                        print(f\"    Type: {source['type']}\")\n",
    "                        print(f\"    URL: {source['url']}\")\n",
    "                        if source['page'] != 'N/A':\n",
    "                            print(f\"    Page: {source['page']}\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(f\"\\n\\n{self.get_cache_stats()}\")\n",
    "                print(\"Exiting...\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac8cc28d-5f2a-42b8-b99a-7fc11cdb8f64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0bf0d2-fe97-464a-ba5e-baf289b4e06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Loading reranker model...\n",
      "Loading generation model: meta-llama/Meta-Llama-3-8B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store...\n",
      "Vector store loaded successfully\n",
      "Note: BM25 index will be created on first hybrid search\n",
      "\n",
      "============================================================\n",
      "Research Assistant - Interactive Mode\n",
      "============================================================\n",
      "Commands:\n",
      "  - Type your question to search local papers\n",
      "  - Add '--online' to search online sources\n",
      "  - Type 'stats' to see cache statistics\n",
      "  - Type 'clear-cache' to clear cache\n",
      "  - Type 'exit' to quit\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  what is multi-head attention and how does it improve transformer model's performance? --online\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing...\n",
      "\n",
      "Searching online sources...\n",
      "Reranking online results...\n",
      "Answer: Multi-head attention is a powerful mechanism that enables Transformer models to capture diverse features, distribute attention effectively, and improve generalization, thereby significantly improving their performance [1]. It works by splitting the self-attention process into several smaller parts, allowing the model to examine information from multiple perspectives simultaneously and process it in parallel [2]. This enhancement leads to improved generalization and adaptability to different tasks and datasets [1], making it a crucial innovation in Transformer models. [References: [1] Understanding the Power and Benefits of Multi-Head Attention in Transformer Models; [2] Multi-head attention significantly improves transformer model performance]\n",
      "\n",
      "Sources:\n",
      "\n",
      "[1] Exploring Multi-Head Attention: Why More Heads Are ...\n",
      "    Type: online_search\n",
      "    URL: https://medium.com/@hassaanidrees7/exploring-multi-head-attention-why-more-heads-are-better-than-one-006a5823372b\n",
      "\n",
      "[2] [FREE] How does multi-head attention in transformers help ...\n",
      "    Type: online_search\n",
      "    URL: https://brainly.com/question/51563320\n",
      "\n",
      "[3] Multi-Head Attention: Why It Outperforms Single- ...\n",
      "    Type: online_search\n",
      "    URL: https://aiml.com/what-is-multi-head-attention-and-how-does-it-improve-model-performance-over-single-attention-head/\n",
      "\n",
      "[4] Multi-Head Attention and Transformer Architecture - Pathway\n",
      "    Type: online_search\n",
      "    URL: https://pathway.com/bootcamps/rag-and-llms/coursework/module-2-word-vectors-simplified/bonus-overview-of-the-transformer-architecture/multi-head-attention-and-transformer-architecture\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  hat is multi-head attention and how does it improve transformer model's performance?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing...\n",
      "\n",
      "Using hybrid search (BM25 + Vector)...\n",
      "Creating BM25 index...\n",
      "BM25 index created\n",
      "Reranking results...\n",
      "Answer: Multi-head attention is an extension of standard self-attention mechanism where multiple parallel attention heads are used simultaneously to jointly attend different representation sub-spaces within the input sequence. By doing so, it enables the model to learn more complex relationships between inputs and outputs, leading to improved performance on various natural language processing tasks. [“The Infini-attention”]\n",
      "\n",
      "Sources:\n",
      "\n",
      "[1] Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2404.07143\n",
      "\n",
      "[2] DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2412.10302\n",
      "\n",
      "[3] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2405.04434\n",
      "\n",
      "[4] Denoising Vision Transformers\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2401.02957\n",
      "\n",
      "[5] RAFT: Adapting Language Model to Domain Specific RAG\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2403.10131\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the most trending prompting technique?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing...\n",
      "\n",
      "Using hybrid search (BM25 + Vector)...\n",
      "Reranking results...\n",
      "Answer: According to the provided research papers, there isn't one specific \"most trending\" prompting technique mentioned explicitly. However, based on the surveys presented in [1] and [2], various studies are being conducted on multiple prompting methods across 29 different NLP tasks over the past few years. This suggests that researchers are actively exploring and experimenting with various approaches to find the most effective ones.\n",
      "\n",
      "Sources:\n",
      "\n",
      "[1] The Prompt Report: A Systematic Survey of Prompting Techniques\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2406.06608\n",
      "\n",
      "[2] A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2407.12994\n",
      "\n",
      "[3] Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2411.00492\n",
      "\n",
      "[4] Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2408.10615\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the most used prompting technique?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing...\n",
      "\n",
      "Using hybrid search (BM25 + Vector)...\n",
      "Reranking results...\n",
      "Answer: The most widely used method to align large language models (LLMs) with human preferences is Reinforcement Learning from Human Feedback (RLHF). This is mentioned in the abstract of the second research paper, \"Reinforcement Learning from Human Feedback\" [2].\n",
      "\n",
      "Sources:\n",
      "\n",
      "[1] A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2407.12994\n",
      "\n",
      "[2] The Prompt Report: A Systematic Survey of Prompting Techniques\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2406.06608\n",
      "\n",
      "[3] Mind Your Step (by Step: Chain-of-Thought Can Reduce Performance on Tasks Where Thinking Makes Humans Worse\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2410.21333\n",
      "\n",
      "[4] Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2404.10719\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========== Usage Example ==========\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize system\n",
    "    # Option 1: Local papers only\n",
    "    rag = HybridResearchRAG(\n",
    "        excel_path=\"LLM-Papers.xlsx\",  # Your Excel file\n",
    "        tavily_api_key= \"\",\n",
    "        vector_store_path=\"research_vectorstore\"\n",
    "    )\n",
    "    \n",
    "    # Option 2: With online search capability\n",
    "    # rag = HybridResearchRAG(\n",
    "    #     excel_path=\"LLM-Papers.xlsx\",\n",
    "    #     tavily_api_key=\"tvly-YOUR-API-KEY\",\n",
    "    #     vector_store_path=\"research_vectorstore\"\n",
    "    # )\n",
    "    \n",
    "    # Start interactive session\n",
    "    rag.interactive_session()\n",
    "    \n",
    "    # Or use programmatically\n",
    "    # result = rag.query(\"What is multi-head attention?\")\n",
    "    # print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9413291e-792e-424c-a167-968e0887d8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4543e53c-5877-4a44-a23b-563702e2ee49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RAG Llama 3)",
   "language": "python",
   "name": "rag_llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
