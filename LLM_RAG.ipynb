{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08d1c9e6-75c2-4dd0-945d-333ce7b8b519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class HybridResearchRAG:\n",
    "    \"\"\"\n",
    "    Unified RAG system supporting both local paper collection and online search.\n",
    "    Optimized for research question-answering with proper source attribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        excel_path: Optional[str] = None,\n",
    "        tavily_api_key: Optional[str] = None,\n",
    "        vector_store_path: str = \"research_vectorstore\",\n",
    "        model_name: str = \"google/flan-t5-large\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid RAG system.\n",
    "        \n",
    "        Args:\n",
    "            excel_path: Path to Excel file with paper metadata (Title, Link, S.No)\n",
    "            tavily_api_key: API key for Tavily web search\n",
    "            vector_store_path: Directory to save/load vector store\n",
    "            model_name: HuggingFace model for generation\n",
    "        \"\"\"\n",
    "        self.vector_store_path = vector_store_path\n",
    "        self.excel_path = excel_path\n",
    "        self.tavily_api_key = tavily_api_key\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        print(\"Loading embedding model...\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={'device': 'cpu'}\n",
    "        )\n",
    "        \n",
    "        # Initialize text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=400, \n",
    "            chunk_overlap=50,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \" \"],\n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "        # Initialize LLM\n",
    "        print(f\"Loading generation model: {model_name}...\")\n",
    "        self.llm = self._initialize_llm(model_name)\n",
    "        \n",
    "        # Initialize vector store\n",
    "        self.vectorstore = None\n",
    "        self.load_or_create_vectorstore()\n",
    "        \n",
    "        # Initialize online retriever if API key provided\n",
    "        self.online_retriever = None\n",
    "        if tavily_api_key:\n",
    "            self.online_retriever = TavilySearchAPIRetriever(\n",
    "                k=8,\n",
    "                api_key=tavily_api_key\n",
    "            )\n",
    "    \n",
    "    def _initialize_llm(self, model_name: str):\n",
    "        \"\"\"Initialize the language model with optimized settings.\"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        \n",
    "        pipe = pipeline(\n",
    "            \"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=400, \n",
    "            temperature=0.5,\n",
    "            repetition_penalty=1.5,\n",
    "            do_sample=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return HuggingFacePipeline(pipeline=pipe)\n",
    "    \n",
    "    def load_papers_from_excel(self) -> List[Dict]:\n",
    "        \"\"\"Load paper metadata from Excel file.\"\"\"\n",
    "        if not self.excel_path or not os.path.exists(self.excel_path):\n",
    "            return []\n",
    "        \n",
    "        df = pd.read_excel(self.excel_path)\n",
    "        return [\n",
    "            {\n",
    "                \"title\": row[\"Title\"],\n",
    "                \"url\": row[\"Link\"],\n",
    "                \"s_no\": row.get(\"S.No\", idx)\n",
    "            }\n",
    "            for idx, row in df.iterrows()\n",
    "        ]\n",
    "    \n",
    "    def load_documents_from_papers(self, papers: List[Dict]) -> List:\n",
    "        \"\"\"Load and process documents from paper URLs.\"\"\"\n",
    "        all_docs = []\n",
    "        total = len(papers)\n",
    "        \n",
    "        print(f\"\\nLoading {total} research papers...\")\n",
    "        \n",
    "        for idx, paper in enumerate(papers, 1):\n",
    "            if idx % 50 == 0:\n",
    "                print(f\"Progress: {idx}/{total} papers loaded\")\n",
    "            \n",
    "            try:\n",
    "                # Choose appropriate loader\n",
    "                if paper[\"url\"].endswith('.pdf'):\n",
    "                    loader = PyMuPDFLoader(paper[\"url\"])\n",
    "                else:\n",
    "                    loader = WebBaseLoader(paper[\"url\"])\n",
    "                \n",
    "                docs = loader.load()\n",
    "                \n",
    "                # Add metadata\n",
    "                for doc in docs:\n",
    "                    doc.metadata.update({\n",
    "                        \"paper_title\": paper[\"title\"],\n",
    "                        \"paper_url\": paper[\"url\"],\n",
    "                        \"s_no\": paper[\"s_no\"],\n",
    "                        \"source_type\": \"local_collection\"\n",
    "                    })\n",
    "                \n",
    "                all_docs.extend(docs)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Log errors but continue\n",
    "                with open(\"load_errors.log\", \"a\") as f:\n",
    "                    f.write(f\"Failed: {paper['title']} - {str(e)}\\n\")\n",
    "        \n",
    "        print(f\"Successfully loaded {len(all_docs)} documents\")\n",
    "        return all_docs\n",
    "    \n",
    "    def load_or_create_vectorstore(self):\n",
    "        \"\"\"Load existing vector store or create new one from local papers.\"\"\"\n",
    "        index_path = f\"{self.vector_store_path}/index.faiss\"\n",
    "        \n",
    "        if os.path.exists(index_path):\n",
    "            print(\"Loading existing vector store...\")\n",
    "            self.vectorstore = FAISS.load_local(\n",
    "                self.vector_store_path,\n",
    "                self.embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            print(\"Vector store loaded successfully\")\n",
    "        elif self.excel_path:\n",
    "            print(\"Creating new vector store from local papers...\")\n",
    "            papers = self.load_papers_from_excel()\n",
    "            if papers:\n",
    "                docs = self.load_documents_from_papers(papers)\n",
    "                chunks = self.text_splitter.split_documents(docs)\n",
    "                \n",
    "                print(f\"Creating embeddings for {len(chunks)} chunks...\")\n",
    "                self.vectorstore = FAISS.from_documents(chunks, self.embeddings)\n",
    "                self.vectorstore.save_local(self.vector_store_path)\n",
    "                print(\"Vector store created and saved\")\n",
    "            else:\n",
    "                print(\"No papers found to process\")\n",
    "        else:\n",
    "            print(\"No vector store or Excel file provided - online search only mode\")\n",
    "    \n",
    "    def search_online(self, query: str) -> List:\n",
    "        \"\"\"Search online sources using Tavily.\"\"\"\n",
    "        if not self.online_retriever:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            print(\"Searching online sources...\")\n",
    "            docs = self.online_retriever.invoke(query)\n",
    "            \n",
    "            # Add metadata and split\n",
    "            for doc in docs:\n",
    "                doc.metadata[\"source_type\"] = \"online_search\"\n",
    "            \n",
    "            chunks = self.text_splitter.split_documents(docs)\n",
    "            return chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Online search error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_qa_chain(self, use_online: bool = False):\n",
    "        \"\"\"Create the QA chain with appropriate retriever.\"\"\"\n",
    "        \n",
    "        # Improved prompt template\n",
    "        prompt_template = \"\"\"You are a research assistant. Answer the question using the provided research papers.\n",
    "\n",
    "Guidelines:\n",
    "1. Provide a clear, concise answer (2-3 sentences maximum)\n",
    "2. Cite specific papers using their titles when possible\n",
    "3. If information is insufficient, state what's available and what's missing\n",
    "4. Be precise and avoid speculation\n",
    "\n",
    "Research Papers:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            template=prompt_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        # Determine retriever\n",
    "        if use_online and self.online_retriever:\n",
    "            # For online searches, we'll handle retrieval manually\n",
    "            return None  # Signal to use custom retrieval\n",
    "        elif self.vectorstore:\n",
    "            retriever = self.vectorstore.as_retriever(\n",
    "                search_kwargs={\"k\": 6}  # Limited for token constraints\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"No retriever available. Provide Excel file or Tavily API key.\")\n",
    "        \n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            chain_type_kwargs={\"prompt\": prompt},\n",
    "            return_source_documents=True\n",
    "        )\n",
    "    \n",
    "    def query(self, question: str, search_online: bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Query the RAG system.\n",
    "        \n",
    "        Args:\n",
    "            question: User question\n",
    "            search_online: Whether to search online sources (requires API key)\n",
    "        \n",
    "        Returns:\n",
    "            Dict with 'answer' and 'sources'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Handle online search separately\n",
    "            if search_online and self.online_retriever:\n",
    "                online_docs = self.search_online(question)\n",
    "                \n",
    "                if online_docs:\n",
    "                    # Create temporary vectorstore\n",
    "                    temp_store = FAISS.from_documents(online_docs, self.embeddings)\n",
    "                    retriever = temp_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "                    \n",
    "                    qa_chain = RetrievalQA.from_chain_type(\n",
    "                        llm=self.llm,\n",
    "                        chain_type=\"stuff\",\n",
    "                        retriever=retriever,\n",
    "                        return_source_documents=True\n",
    "                    )\n",
    "                    \n",
    "                    response = qa_chain.invoke({\"query\": question})\n",
    "                else:\n",
    "                    return {\n",
    "                        \"answer\": \"No relevant online sources found.\",\n",
    "                        \"sources\": []\n",
    "                    }\n",
    "            else:\n",
    "                # Use local vectorstore\n",
    "                qa_chain = self.create_qa_chain(use_online=False)\n",
    "                response = qa_chain.invoke({\"query\": question})\n",
    "            \n",
    "            # Extract answer\n",
    "            answer = response.get(\"result\", \"No answer generated\")\n",
    "            \n",
    "            # Process sources\n",
    "            sources = []\n",
    "            seen = set()\n",
    "            \n",
    "            for doc in response.get(\"source_documents\", []):\n",
    "                source_info = {\n",
    "                    \"title\": doc.metadata.get(\"paper_title\", \"Unknown\"),\n",
    "                    \"url\": doc.metadata.get(\"paper_url\", doc.metadata.get(\"source\", \"#\")),\n",
    "                    \"type\": doc.metadata.get(\"source_type\", \"unknown\"),\n",
    "                    \"page\": doc.metadata.get(\"page\", \"N/A\")\n",
    "                }\n",
    "                \n",
    "                # Deduplicate\n",
    "                identifier = f\"{source_info['title']}_{source_info['url']}\"\n",
    "                if identifier not in seen:\n",
    "                    sources.append(source_info)\n",
    "                    seen.add(identifier)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"sources\": sources\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"Error processing query: {str(e)}\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "    \n",
    "    def interactive_session(self):\n",
    "        \"\"\"Run interactive Q&A session.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Research Assistant - Interactive Mode\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Commands:\")\n",
    "        print(\"  - Type your question to search local papers\")\n",
    "        print(\"  - Add '--online' to search online sources\")\n",
    "        print(\"  - Type 'exit' to quit\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nQuestion: \").strip()\n",
    "                \n",
    "                if user_input.lower() in ['exit', 'quit', 'q']:\n",
    "                    print(\"Goodbye!\")\n",
    "                    break\n",
    "                \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                # Check for online flag\n",
    "                search_online = '--online' in user_input\n",
    "                question = user_input.replace('--online', '').strip()\n",
    "                \n",
    "                print(\"\\nProcessing...\\n\")\n",
    "                result = self.query(question, search_online=search_online)\n",
    "                \n",
    "                # Display results\n",
    "                print(f\"Answer: {result['answer']}\\n\")\n",
    "                \n",
    "                if result['sources']:\n",
    "                    print(\"Sources:\")\n",
    "                    for idx, source in enumerate(result['sources'], 1):\n",
    "                        print(f\"\\n[{idx}] {source['title']}\")\n",
    "                        print(f\"    Type: {source['type']}\")\n",
    "                        print(f\"    URL: {source['url']}\")\n",
    "                        if source['page'] != 'N/A':\n",
    "                            print(f\"    Page: {source['page']}\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nExiting...\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "934635a6-d33b-4c68-af38-734d2e4ee228",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Loading generation model: google/flan-t5-large...\n",
      "Loading existing vector store...\n",
      "Vector store loaded successfully\n",
      "\n",
      "============================================================\n",
      "Research Assistant - Interactive Mode\n",
      "============================================================\n",
      "Commands:\n",
      "  - Type your question to search local papers\n",
      "  - Add '--online' to search online sources\n",
      "  - Type 'exit' to quit\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the most trending prompting technique?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing...\n",
      "\n",
      "Answer: Multi-expert Prompting\n",
      "\n",
      "Sources:\n",
      "\n",
      "[1] The Prompt Report: A Systematic Survey of Prompting Techniques\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2406.06608\n",
      "\n",
      "[2] A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2407.12994\n",
      "\n",
      "[3] Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2411.00492\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is the most trending prompting technique? --online\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing...\n",
      "\n",
      "Searching online sources...\n",
      "Answer: Meta-prompting\n",
      "\n",
      "Sources:\n",
      "\n",
      "[1] Unknown\n",
      "    Type: online_search\n",
      "    URL: https://www.reddit.com/r/PromptEngineering/comments/1ius9pt/my_favorite_prompting_technique_whats_yours/\n",
      "\n",
      "[2] Unknown\n",
      "    Type: online_search\n",
      "    URL: https://www.promptingguide.ai/techniques\n",
      "\n",
      "[3] Unknown\n",
      "    Type: online_search\n",
      "    URL: https://www.linkedin.com/pulse/mastering-advanced-prompting-techniques-large-language-watkins-lik9e\n",
      "\n",
      "[4] Unknown\n",
      "    Type: online_search\n",
      "    URL: https://machinelearningmastery.com/7-next-generation-prompt-engineering-techniques/\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is multi-head attention?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing...\n",
      "\n",
      "Answer: The multi-head mechanism enables the model to collectively attend to information from various representation spaces within different experts, while significantly enhances expert activation, thus deepens context understanding and alleviate overfitting\n",
      "\n",
      "Sources:\n",
      "\n",
      "[1] Attention Heads of Large Language Models: A Survey\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2409.03752\n",
      "\n",
      "[2] Retrieval Head Mechanistically Explains Long-Context Factuality\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2404.15574\n",
      "\n",
      "[3] Multi-Head Mixture-of-Experts\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2404.15045\n",
      "\n",
      "[4] MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2406.14909\n",
      "\n",
      "[5] A Phase Transition Between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention\n",
      "    Type: local_collection\n",
      "    URL: https://arxiv.org/abs/2402.03902\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========== Usage Example ==========\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize system\n",
    "    # Option 1: Local papers only\n",
    "    rag = HybridResearchRAG(\n",
    "        excel_path=\"LLM-Papers.xlsx\",  # Your Excel file\n",
    "        tavily_api_key= \"\",\n",
    "        vector_store_path=\"research_vectorstore\"\n",
    "    )\n",
    "    \n",
    "    # Option 2: With online search capability\n",
    "    # rag = HybridResearchRAG(\n",
    "    #     excel_path=\"LLM-Papers.xlsx\",\n",
    "    #     tavily_api_key=\"tvly-YOUR-API-KEY\",\n",
    "    #     vector_store_path=\"research_vectorstore\"\n",
    "    # )\n",
    "    \n",
    "    # Start interactive session\n",
    "    rag.interactive_session()\n",
    "    \n",
    "    # Or use programmatically\n",
    "    # result = rag.query(\"What is multi-head attention?\")\n",
    "    # print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1167303-1b75-46af-a0b3-97682df80857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
